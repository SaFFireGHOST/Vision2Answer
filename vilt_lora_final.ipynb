{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering (VQA) Fine-Tuning with LoRA\n",
    "\n",
    "This notebook implements fine-tuning of the **ViLT** model for Visual Question Answering (VQA) using **Low-Rank Adaptation (LoRA)** on a dataset split into training and test sets. The code includes data loading, model setup with LoRA, training, evaluation, and inference, with results saved for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "The following steps outline the initial setup:\n",
    "\n",
    "- **Import libraries**  \n",
    "  Includes `torch`, `transformers`, `peft`, `PIL`, `pandas`, `sklearn`, and others for model fine-tuning and data processing.\n",
    "\n",
    "- **Define file paths**  \n",
    "  Specifies locations for images, VQA dataset, metadata, model output, and results.\n",
    "\n",
    "- **Set image size**  \n",
    "  Uses `384x384` as required by ViLT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with train and test split fine tuning with lora for vilt model on entire dataset.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "IMAGE_BASE_DIR = r\"/kaggle/input/vr-project/images/small\"\n",
    "VQA_DATA_FILE = \"/kaggle/input/vqa-training-complete/vqa_training_data_complete.json\"\n",
    "INPUT_IMAGES_FILE = \"/kaggle/input/vr-project/images/metadata/images.csv\"\n",
    "OUTPUT_MODEL_DIR = \"/kaggle/working/vilt-lora-finetuned\"\n",
    "OUTPUT_RESULTS_FILE = \"/kaggle/working/vqa_finetune_results.csv\"\n",
    "\n",
    "# Fixed image size for ViLT\n",
    "IMAGE_SIZE = (384, 384)  # ViLT expects 384x384 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom VQA Dataset\n",
    "\n",
    "The `VQADataset` class handles the VQA dataset:\n",
    "\n",
    "- **Initialization**  \n",
    "  Takes VQA data, image metadata, processor, and answer-to-index mapping.\n",
    "\n",
    "- **Data loading**  \n",
    "  Loads and preprocesses images and questions.\n",
    "\n",
    "- **Output**  \n",
    "  Returns encoded inputs (image, text, labels) for model training.\n",
    "\n",
    "The `custom_collate_fn` ensures proper batching of tensor inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom VQA Dataset\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, vqa_data, image_map, processor, answer_to_idx):\n",
    "        self.vqa_data = vqa_data\n",
    "        self.image_map = image_map\n",
    "        self.processor = processor\n",
    "        self.answer_to_idx = answer_to_idx\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor()  # Outputs (C, H, W) with values in [0, 1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(item[\"questions\"]) for item in self.vqa_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find the correct item and question\n",
    "        count = 0\n",
    "        for item in self.vqa_data:\n",
    "            for q_item in item[\"questions\"]:\n",
    "                if count == idx:\n",
    "                    image_id = item[\"image_id\"]\n",
    "                    question = q_item[\"question\"]\n",
    "                    answer = q_item[\"answer\"]\n",
    "                    break\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(IMAGE_BASE_DIR, self.image_map[image_id][\"path\"])\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Apply resizing transform\n",
    "        image_tensor = self.transform(image)  # Shape: (C, H, W)\n",
    "\n",
    "        # Process inputs with ViLT processor\n",
    "        encoding = self.processor(\n",
    "            images=image_tensor,\n",
    "            text=question,\n",
    "            padding=\"max_length\",\n",
    "            max_length=40,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Prepare labels - using one-hot encoding for VQA classification\n",
    "        answer_idx = self.answer_to_idx.get(answer, -1)\n",
    "        \n",
    "        # Create a one-hot tensor with same size as model output logits\n",
    "        # The size should match the model's output size (3129 in this case)\n",
    "        num_answers = len(self.answer_to_idx)\n",
    "        if answer_idx != -1:\n",
    "            # Create a one-hot encoded label\n",
    "            one_hot = torch.zeros(num_answers)\n",
    "            one_hot[answer_idx] = 1.0\n",
    "            encoding[\"labels\"] = one_hot\n",
    "        else:\n",
    "            # Handle unknown answers\n",
    "            encoding[\"labels\"] = torch.zeros(num_answers)\n",
    "\n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        return encoding\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle ViLT inputs, only stacking tensors.\"\"\"\n",
    "    keys = batch[0].keys()\n",
    "    collated = {}\n",
    "\n",
    "    for key in keys:\n",
    "        items = [item[key] for item in batch]\n",
    "        if all(isinstance(item, torch.Tensor) for item in items):\n",
    "            try:\n",
    "                collated[key] = torch.stack(items)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error stacking key '{key}': {e}\")\n",
    "                print(f\"Shapes: {[item.shape for item in items]}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"Skipping key '{key}' as it contains non-tensor items: {type(items[0])}\")\n",
    "            continue  # Skip non-tensor keys\n",
    "\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "These functions prepare the dataset:\n",
    "\n",
    "- **`load_image_metadata`**  \n",
    "  Loads image metadata from a CSV into a dictionary.\n",
    "\n",
    "- **`load_vqa_data`**  \n",
    "  Loads the VQA dataset from a JSON file.\n",
    "\n",
    "- **`create_answer_to_idx`**  \n",
    "  Creates a mapping from answers to indices based on frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_metadata():\n",
    "    \"\"\"Load image metadata from CSV into a dictionary.\"\"\"\n",
    "    image_map = {}\n",
    "    with open(INPUT_IMAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            image_map[row[\"image_id\"]] = {\n",
    "                \"height\": int(row[\"height\"]),\n",
    "                \"width\": int(row[\"width\"]),\n",
    "                \"path\": row[\"path\"]\n",
    "            }\n",
    "    return image_map\n",
    "\n",
    "def load_vqa_data():\n",
    "    \"\"\"Load the VQA dataset from JSON.\"\"\"\n",
    "    with open(VQA_DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        vqa_data = json.load(f)\n",
    "    return vqa_data\n",
    "\n",
    "def create_answer_to_idx(vqa_data):\n",
    "    \"\"\"Create a mapping from answers to indices based on frequency.\"\"\"\n",
    "    answer_freq = {}\n",
    "    for item in vqa_data:\n",
    "        for q_item in item[\"questions\"]:\n",
    "            answer = q_item[\"answer\"]\n",
    "            answer_freq[answer] = answer_freq.get(answer, 0) + 1\n",
    "\n",
    "    # Print diagnostic info\n",
    "    print(f\"Total unique answers in dataset: {len(answer_freq)}\")\n",
    "    \n",
    "    # Use model's vocabulary size - we'll get this later\n",
    "    # For now, keep most frequent answers\n",
    "    sorted_answers = sorted(answer_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Print top 5 answers for diagnostic purposes\n",
    "    print(\"Top 5 answers by frequency:\")\n",
    "    for i, (answer, count) in enumerate(sorted_answers[:5]):\n",
    "        print(f\"  {i}. '{answer}': {count} occurrences\")\n",
    "        \n",
    "    return {answer: idx for idx, (answer, _) in enumerate(sorted_answers)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup with LoRA\n",
    "\n",
    "The `setup_lora_model` function initializes the ViLT model with LoRA:\n",
    "\n",
    "- Loads the pre-trained ViLT model and processor.\n",
    "- Applies LoRA configuration with specified parameters.\n",
    "- Ensures trainable parameters are within limits.\n",
    "- Moves the model to GPU if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lora_model():\n",
    "    \"\"\"Initialize ViLT model with LoRA configuration.\"\"\"\n",
    "    processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "    # Disable image rescaling at processor initialization\n",
    "    processor.image_processor.do_rescale = False\n",
    "    model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "    \n",
    "    # Store the original classifier size\n",
    "    original_num_labels = model.config.num_labels\n",
    "    print(f\"Original model has {original_num_labels} answer classes\")\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "    if trainable_params > 7_000_000:\n",
    "        raise ValueError(\"Trainable parameters exceed 7M limit\")\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return processor, model, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "These functions handle model training and evaluation:\n",
    "\n",
    "- **`train_model`**  \n",
    "  Fine-tunes the LoRA-adapted model using AdamW optimizer and CrossEntropyLoss.\n",
    "\n",
    "- **`evaluate_model`**  \n",
    "  Evaluates the model on the test set, computing loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, processor, train_dataloader, device, num_epochs=3):\n",
    "    \"\"\"Train the LoRA-finetuned ViLT model.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Added explicit loss function\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            try:\n",
    "                # Verify batch contains only tensors\n",
    "                for key, value in batch.items():\n",
    "                    if not isinstance(value, torch.Tensor):\n",
    "                        raise ValueError(f\"Batch key '{key}' is not a tensor: {type(value)}\")\n",
    "\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Extract labels before passing to model\n",
    "                labels = batch.pop(\"labels\")\n",
    "                \n",
    "                # Forward pass - compute outputs without labels\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits  # Shape [batch_size, num_answers]\n",
    "                \n",
    "                # Compute loss manually using cross entropy\n",
    "                loss = criterion(logits, labels)  # CrossEntropyLoss expects logits and class indices\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch: {str(e)}\")\n",
    "                print(f\"Batch keys: {list(batch.keys())}\")\n",
    "                continue\n",
    "\n",
    "        if batch_count > 0:\n",
    "            avg_loss = total_loss / batch_count\n",
    "            print(f\"Average loss for epoch {epoch + 1}: {avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"No valid batches processed in epoch {epoch + 1}\")\n",
    "\n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    \"\"\"Evaluate the model on test data.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Extract labels before passing to model\n",
    "                labels = batch.pop(\"labels\")\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                _, target = torch.max(labels, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation batch: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_loss = test_loss / len(test_dataloader) if len(test_dataloader) > 0 else 0\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "The `run_inference` function performs inference on the test set:\n",
    "\n",
    "- Generates predictions for each question-image pair.\n",
    "- Compares predictions to ground truth.\n",
    "- Saves results to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, processor, test_data, image_map, answer_to_idx, device):\n",
    "    \"\"\"Run inference on the test dataset and save results.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(test_data, desc=\"Processing test VQA items\"):\n",
    "            image_id = item[\"image_id\"]\n",
    "            if image_id not in image_map:\n",
    "                print(f\"Warning: Image ID {image_id} not found in metadata. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(IMAGE_BASE_DIR, image_map[image_id][\"path\"])\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image file not found at {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize(IMAGE_SIZE),\n",
    "                    transforms.ToTensor()\n",
    "                ])\n",
    "                \n",
    "                for q_item in item[\"questions\"]:\n",
    "                    question = q_item[\"question\"]\n",
    "                    ground_truth = q_item[\"answer\"]\n",
    "                    ground_truth_idx = answer_to_idx.get(ground_truth, -1)\n",
    "\n",
    "                    # Preprocess image\n",
    "                    image_tensor = transform(image)\n",
    "\n",
    "                    # Prepare inputs\n",
    "                    inputs = processor(\n",
    "                        images=image_tensor,\n",
    "                        text=question,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=40,\n",
    "                        truncation=True\n",
    "                    ).to(device)\n",
    "\n",
    "                    # Generate answer\n",
    "                    outputs = model(**inputs)\n",
    "                    predicted_answer_idx = outputs.logits.argmax(-1).item()\n",
    "                    \n",
    "                    # Convert index back to answer text\n",
    "                    idx_to_answer = {idx: answer for answer, idx in answer_to_idx.items()}\n",
    "                    predicted_answer = idx_to_answer.get(predicted_answer_idx, f\"unknown_{predicted_answer_idx}\")\n",
    "\n",
    "                    # Store result\n",
    "                    results.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"question\": question,\n",
    "                        \"ground_truth\": ground_truth,\n",
    "                        \"predicted_answer\": predicted_answer,\n",
    "                        \"correct\": predicted_answer == ground_truth\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for r in results if r[\"correct\"]) / len(results) if results else 0\n",
    "    print(f\"Inference accuracy: {accuracy:.4f} ({sum(1 for r in results if r['correct'])}/{len(results)})\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(OUTPUT_RESULTS_FILE, index=False)\n",
    "    print(f\"Results saved to {OUTPUT_RESULTS_FILE}\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "The `main` function orchestrates the entire process:\n",
    "\n",
    "- Loads and splits the dataset into training and test sets.\n",
    "- Sets up the model with LoRA.\n",
    "- Creates data loaders for training and testing.\n",
    "- Trains and evaluates the model.\n",
    "- Saves the model and runs inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T09:53:51.857920Z",
     "iopub.status.busy": "2025-05-13T09:53:51.857572Z",
     "iopub.status.idle": "2025-05-13T10:39:06.065462Z",
     "shell.execute_reply": "2025-05-13T10:39:06.064857Z",
     "shell.execute_reply.started": "2025-05-13T09:53:51.857892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 7094 items\n",
      "Test data: 1774 items\n",
      "Original model has 3129 answer classes\n",
      "Number of trainable parameters: 589824\n",
      "Total unique answers in dataset: 2093\n",
      "Top 5 answers by frequency:\n",
      "  0. 'Yes': 3060 occurrences\n",
      "  1. 'Black': 1391 occurrences\n",
      "  2. 'Two': 1234 occurrences\n",
      "  3. 'Blue': 968 occurrences\n",
      "  4. 'Hard': 739 occurrences\n",
      "Model expects 3129 possible answers\n",
      "Warning: Answer mapping size (2093) doesn't match model's vocabulary (3129)\n",
      "Using model's id2label mapping instead\n",
      "Model's answer vocabulary size: 3129\n",
      "Training model on training data...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2433/2433 [13:38<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 1: 0.0899\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2433/2433 [12:49<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 2: 0.0711\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2433/2433 [12:33<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 3: 0.0611\n",
      "Evaluating model on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 606/606 [02:35<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0658\n",
      "Test Accuracy: 0.0076 (37/4848)\n",
      "Model saved to /kaggle/working/vilt-lora-finetuned\n",
      "Running inference on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test VQA items: 100%|██████████| 1774/1774 [03:34<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference accuracy: 0.0076 (37/4848)\n",
      "Results saved to /kaggle/working/vqa_finetune_results.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    image_map = load_image_metadata()\n",
    "    vqa_data = load_vqa_data()\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    train_data, test_data = train_test_split(vqa_data, test_size=0.2, random_state=42)\n",
    "    print(f\"Training data: {len(train_data)} items\")\n",
    "    print(f\"Test data: {len(test_data)} items\")\n",
    "    \n",
    "    # Setup model first to get the proper answer vocabulary size\n",
    "    processor, model, device = setup_lora_model()\n",
    "    \n",
    "    # Now get the answer-to-index mapping from the full dataset\n",
    "    # (We need the full vocabulary, even if we're only training on a subset)\n",
    "    answer_to_idx = create_answer_to_idx(vqa_data)\n",
    "    \n",
    "    # Get the vocabulary size from the model\n",
    "    num_labels = model.config.num_labels\n",
    "    print(f\"Model expects {num_labels} possible answers\")\n",
    "    \n",
    "    # Ensure our answer mapping matches the model's expected vocabulary size\n",
    "    if len(answer_to_idx) != num_labels:\n",
    "        print(f\"Warning: Answer mapping size ({len(answer_to_idx)}) doesn't match model's vocabulary ({num_labels})\")\n",
    "        print(\"Using model's id2label mapping instead\")\n",
    "        answer_to_idx = {v: int(k) for k, v in model.config.id2label.items()}\n",
    "        print(f\"Model's answer vocabulary size: {len(answer_to_idx)}\")\n",
    "    \n",
    "    # Create training dataset and dataloader\n",
    "    train_dataset = VQADataset(train_data, image_map, processor, answer_to_idx)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Keep at 0 for debugging\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    # Create test dataset and dataloader for evaluation\n",
    "    test_dataset = VQADataset(test_data, image_map, processor, answer_to_idx)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training model on training data...\")\n",
    "    train_model(model, processor, train_dataloader, device)\n",
    "\n",
    "    # Evaluate model on test data\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "    # Save model\n",
    "    model.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "    processor.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "    print(f\"Model saved to {OUTPUT_MODEL_DIR}\")\n",
    "\n",
    "    # Run inference on test data\n",
    "    print(\"Running inference on test data...\")\n",
    "    results_df = run_inference(model, processor, test_data, image_map, answer_to_idx, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering (VQA) Evaluation Part\n",
    "\n",
    "This Part evaluates the performance of a Visual Question Answering (VQA) model by computing various metrics on a results dataset. It calculates Exact Match, Token Match, Wu-Palmer (WUP) Score, F1 Score, and BERTScore, and provides detailed analysis by question type and for yes/no questions.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "The following steps outline the initial setup:\n",
    "\n",
    "- **Import libraries**  \n",
    "  Includes `pandas`, `nltk`, `transformers`, `torch`, `sklearn`, and others for data processing and metric computation.\n",
    "\n",
    "- **Download NLTK resources**  \n",
    "  Ensures WordNet, Punkt, and POS tagger are available for text processing.\n",
    "\n",
    "- **Define output path**  \n",
    "  Specifies the location for saving metrics.\n",
    "\n",
    "- **Suppress warnings**  \n",
    "  Ignores non-critical warnings to keep output clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from nltk.corpus import wordnet as wn\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "OUTPUT_METRICS_FILE = \"/kaggle/working/vqa_metrics.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization and Scoring Functions\n",
    "\n",
    "These functions handle text processing and metric calculations:\n",
    "\n",
    "- **`normalize_answer`**  \n",
    "  Converts answers to a standardized format by mapping digits to words, removing articles, punctuation, and extra whitespace.\n",
    "\n",
    "- **`calculate_bertscore`**  \n",
    "  Computes BERTScore using BERT embeddings and cosine similarity.\n",
    "\n",
    "- **`exact_match`**  \n",
    "  Checks if the predicted answer exactly matches the ground truth after normalization.\n",
    "\n",
    "- **`token_match`**  \n",
    "  Compares token sets between predicted and ground truth answers.\n",
    "\n",
    "- **`get_wordnet_pos`**  \n",
    "  Maps NLTK POS tags to WordNet POS for WUP similarity.\n",
    "\n",
    "- **`calculate_wup_score`**  \n",
    "  Calculates Wu-Palmer similarity between predicted and ground truth answers.\n",
    "\n",
    "- **`calculate_f1_score`**  \n",
    "  Computes F1 score based on token overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    if pd.isna(s) or not isinstance(s, str):\n",
    "        s = \"\"\n",
    "    number_map = {\n",
    "        '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "        '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine', '10': 'ten'\n",
    "    }\n",
    "    s = str(s).lower()\n",
    "    for digit, word in number_map.items():\n",
    "        s = re.sub(r'\\b' + digit + r'\\b', word, s)\n",
    "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def calculate_bertscore(pred, ref, tokenizer, model, device):\n",
    "    pred = normalize_answer(pred)\n",
    "    ref = normalize_answer(ref)\n",
    "    if not pred or not ref:\n",
    "        return 0.0\n",
    "    pred_tokens = tokenizer(pred, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    ref_tokens = tokenizer(ref, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred_outputs = model(**pred_tokens)\n",
    "        ref_outputs = model(**ref_tokens)\n",
    "    pred_embedding = pred_outputs.last_hidden_state[:, 0, :]\n",
    "    ref_embedding = ref_outputs.last_hidden_state[:, 0, :]\n",
    "    pred_embedding = pred_embedding / pred_embedding.norm(dim=1, keepdim=True)\n",
    "    ref_embedding = ref_embedding / ref_embedding.norm(dim=1, keepdim=True)\n",
    "    similarity = torch.matmul(pred_embedding, ref_embedding.transpose(0, 1)).item()\n",
    "    return similarity\n",
    "\n",
    "def exact_match(pred, ref):\n",
    "    return normalize_answer(pred) == normalize_answer(ref)\n",
    "\n",
    "def token_match(pred, ref):\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    ref_tokens = normalize_answer(ref).split()\n",
    "    return Counter(pred_tokens) == Counter(ref_tokens)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wn.ADJ, \"N\": wn.NOUN, \"V\": wn.VERB, \"R\": wn.ADV}\n",
    "    return tag_dict.get(tag, wn.NOUN)\n",
    "\n",
    "def calculate_wup_score(pred, ref):\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    ref_tokens = normalize_answer(ref).split()\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    max_similarities = []\n",
    "    for p_token in pred_tokens:\n",
    "        token_max_sim = 0.0\n",
    "        p_synsets = wn.synsets(p_token, pos=get_wordnet_pos(p_token))\n",
    "        if not p_synsets:\n",
    "            p_synsets = wn.synsets(p_token)\n",
    "        if not p_synsets:\n",
    "            continue\n",
    "        for r_token in ref_tokens:\n",
    "            r_synsets = wn.synsets(r_token, pos=get_wordnet_pos(r_token))\n",
    "            if not r_synsets:\n",
    "                r_synsets = wn.synsets(r_token)\n",
    "            if not r_synsets:\n",
    "                continue\n",
    "            token_sims = []\n",
    "            for p_syn in p_synsets:\n",
    "                for r_syn in r_synsets:\n",
    "                    try:\n",
    "                        sim = wn.wup_similarity(p_syn, r_syn)\n",
    "                        if sim is not None:\n",
    "                            token_sims.append(sim)\n",
    "                    except:\n",
    "                        continue\n",
    "            if token_sims:\n",
    "                token_max_sim = max(token_max_sim, max(token_sims))\n",
    "        if token_max_sim > 0:\n",
    "            max_similarities.append(token_max_sim)\n",
    "    return sum(max_similarities) / len(max_similarities) if max_similarities else 0.0\n",
    "\n",
    "def calculate_f1_score(pred, ref):\n",
    "    pred_tokens = set(normalize_answer(pred).split())\n",
    "    ref_tokens = set(normalize_answer(ref).split())\n",
    "    if not pred_tokens and not ref_tokens:\n",
    "        return 1.0\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    common_tokens = pred_tokens.intersection(ref_tokens)\n",
    "    precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0.0\n",
    "    recall = len(common_tokens) / len(ref_tokens) if ref_tokens else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore Model Setup\n",
    "\n",
    "The `setup_bertscore_model` function initializes the BERT model for BERTScore calculation:\n",
    "\n",
    "- Loads the BERT tokenizer and model (`bert-base-uncased`).\n",
    "- Moves the model to GPU if available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def setup_bertscore_model():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return tokenizer, model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_bertscore_model():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return tokenizer, model, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "The `evaluate_results` function computes and analyzes metrics:\n",
    "\n",
    "- Calculates Exact Match, Token Match, WUP Score, F1 Score, and BERTScore for each prediction.\n",
    "- Categorizes questions into types (counting, color, yes/no, other).\n",
    "- Computes metrics by question type.\n",
    "- Analyzes yes/no questions with binary classification metrics (accuracy, precision, recall, F1).\n",
    "- Saves metrics to a JSON file and prints a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results_df):\n",
    "    bert_tokenizer, bert_model, bert_device = setup_bertscore_model()\n",
    "    \n",
    "    # Compute metrics for each row\n",
    "    results_df[\"exact_match\"] = results_df.apply(lambda row: exact_match(row[\"predicted_answer\"], row[\"ground_truth\"]), axis=1)\n",
    "    results_df[\"token_match\"] = results_df.apply(lambda row: token_match(row[\"predicted_answer\"], row[\"ground_truth\"]), axis=1)\n",
    "    results_df[\"wup_score\"] = results_df.apply(lambda row: calculate_wup_score(row[\"predicted_answer\"], row[\"ground_truth\"]), axis=1)\n",
    "    results_df[\"f1_score\"] = results_df.apply(lambda row: calculate_f1_score(row[\"predicted_answer\"], row[\"ground_truth\"]), axis=1)\n",
    "    results_df[\"bertscore\"] = results_df.apply(lambda row: calculate_bertscore(row[\"predicted_answer\"], row[\"ground_truth\"], bert_tokenizer, bert_model, bert_device), axis=1)\n",
    "    \n",
    "    metrics = {\n",
    "        \"overall\": {\n",
    "            \"exact_match\": results_df[\"exact_match\"].mean(),\n",
    "            \"token_match\": results_df[\"token_match\"].mean(),\n",
    "            \"wup_score\": results_df[\"wup_score\"].mean(),\n",
    "            \"f1_score\": results_df[\"f1_score\"].mean(),\n",
    "            \"bertscore\": results_df[\"bertscore\"].mean()\n",
    "        }\n",
    "    }\n",
    "    results_df[\"question_type\"] = \"other\"\n",
    "    results_df.loc[results_df[\"question\"].str.contains(\"how many|number|count\", case=False), \"question_type\"] = \"counting\"\n",
    "    results_df.loc[results_df[\"question\"].str.contains(\"color|colour\", case=False), \"question_type\"] = \"color\"\n",
    "    results_df.loc[results_df[\"question\"].str.startswith((\"Is \", \"Are \", \"Does \", \"Do \", \"Can \", \"Could \", \"Has \", \"Have \")), \"question_type\"] = \"yes/no\"\n",
    "    \n",
    "    question_types = results_df[\"question_type\"].unique()\n",
    "    metrics[\"by_question_type\"] = {}\n",
    "    for qtype in question_types:\n",
    "        subset = results_df[results_df[\"question_type\"] == qtype]\n",
    "        metrics[\"by_question_type\"][qtype] = {\n",
    "            \"count\": len(subset),\n",
    "            \"exact_match\": subset[\"exact_match\"].mean(),\n",
    "            \"token_match\": subset[\"token_match\"].mean(),\n",
    "            \"wup_score\": subset[\"wup_score\"].mean(),\n",
    "            \"f1_score\": subset[\"f1_score\"].mean(),\n",
    "            \"bertscore\": subset[\"bertscore\"].mean()\n",
    "        }\n",
    "    \n",
    "    yes_no_df = results_df[results_df[\"question_type\"] == \"yes/no\"]\n",
    "    if len(yes_no_df) > 0:\n",
    "        yes_no_df[\"gt_binary\"] = yes_no_df[\"ground_truth\"].str.lower().apply(\n",
    "            lambda x: 1 if x in [\"yes\", \"yeah\", \"true\"] else 0)\n",
    "        yes_no_df[\"pred_binary\"] = yes_no_df[\"predicted_answer\"].str.lower().apply(\n",
    "            lambda x: 1 if x in [\"yes\", \"yeah\", \"true\"] else 0)\n",
    "        metrics[\"yes_no_analysis\"] = {\n",
    "            \"accuracy\": (yes_no_df[\"gt_binary\"] == yes_no_df[\"pred_binary\"]).mean(),\n",
    "            \"precision\": precision_score(yes_no_df[\"gt_binary\"], yes_no_df[\"pred_binary\"], zero_division=0),\n",
    "            \"recall\": recall_score(yes_no_df[\"gt_binary\"], yes_no_df[\"pred_binary\"], zero_division=0),\n",
    "            \"f1\": f1_score(yes_no_df[\"gt_binary\"], yes_no_df[\"pred_binary\"], zero_division=0)\n",
    "        }\n",
    "    \n",
    "    with open(OUTPUT_METRICS_FILE, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(\"\\n===== VQA Evaluation Results =====\")\n",
    "    print(f\"Total questions evaluated: {len(results_df)}\")\n",
    "    print(f\"Exact match accuracy: {metrics['overall']['exact_match']:.4f}\")\n",
    "    print(f\"Token match accuracy: {metrics['overall']['token_match']:.4f}\")\n",
    "    print(f\"Average WUP score: {metrics['overall']['wup_score']:.4f}\")\n",
    "    print(f\"Average F1 score: {metrics['overall']['f1_score']:.4f}\")\n",
    "    print(f\"Average BERTScore: {metrics['overall']['bertscore']:.4f}\")\n",
    "    \n",
    "    print(\"\\n===== Results by Question Type =====\")\n",
    "    for qtype, qmetrics in metrics[\"by_question_type\"].items():\n",
    "        print(f\"\\n{qtype.upper()} Questions ({qmetrics['count']} questions):\")\n",
    "        print(f\"  Exact match: {qmetrics['exact_match']:.4f}\")\n",
    "        print(f\"  Token match: {qmetrics['token_match']:.4f}\")\n",
    "        print(f\"  WUP score: {qmetrics['wup_score']:.4f}\")\n",
    "        print(f\"  F1 score: {qmetrics['f1_score']:.4f}\")\n",
    "        print(f\"  BERTScore: {qmetrics['bertscore']:.4f}\")\n",
    "    \n",
    "    if \"yes_no_analysis\" in metrics:\n",
    "        print(\"\\n===== Yes/No Question Analysis =====\")\n",
    "        print(f\"  Accuracy: {metrics['yes_no_analysis']['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['yes_no_analysis']['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['yes_no_analysis']['recall']:.4f}\")\n",
    "        print(f\"  F1 score: {metrics['yes_no_analysis']['f1']:.4f}\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "The main block loads the results and runs the evaluation:\n",
    "\n",
    "- Reads the VQA results from a CSV file.\n",
    "- Calls the `evaluate_results` function to compute and save metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T10:52:50.948550Z",
     "iopub.status.busy": "2025-05-13T10:52:50.948331Z",
     "iopub.status.idle": "2025-05-13T10:54:19.144894Z",
     "shell.execute_reply": "2025-05-13T10:54:19.144022Z",
     "shell.execute_reply.started": "2025-05-13T10:52:50.948533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VQA Evaluation Results =====\n",
      "Total questions evaluated: 4848\n",
      "Exact match accuracy: 0.4099\n",
      "Token match accuracy: 0.4099\n",
      "Average WUP score: 0.6925\n",
      "Average F1 score: 0.4137\n",
      "Average BERTScore: 0.9590\n",
      "\n",
      "===== Results by Question Type =====\n",
      "\n",
      "COLOR Questions (1577 questions):\n",
      "  Exact match: 0.5568\n",
      "  Token match: 0.5568\n",
      "  WUP score: 0.7692\n",
      "  F1 score: 0.5637\n",
      "  BERTScore: 0.9762\n",
      "\n",
      "YES/NO Questions (1195 questions):\n",
      "  Exact match: 0.6126\n",
      "  Token match: 0.6126\n",
      "  WUP score: 0.7268\n",
      "  F1 score: 0.6126\n",
      "  BERTScore: 0.9761\n",
      "\n",
      "OTHER Questions (1441 questions):\n",
      "  Exact match: 0.1443\n",
      "  Token match: 0.1443\n",
      "  WUP score: 0.4962\n",
      "  F1 score: 0.1498\n",
      "  BERTScore: 0.9249\n",
      "\n",
      "COUNTING Questions (635 questions):\n",
      "  Exact match: 0.2661\n",
      "  Token match: 0.2661\n",
      "  WUP score: 0.8833\n",
      "  F1 score: 0.2661\n",
      "  BERTScore: 0.9612\n",
      "\n",
      "===== Yes/No Question Analysis =====\n",
      "  Accuracy: 0.7941\n",
      "  Precision: 0.8127\n",
      "  Recall: 0.7754\n",
      "  F1 score: 0.7936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = pd.read_csv(\"/kaggle/working/vqa_finetune_results.csv\")\n",
    "    evaluate_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7379845,
     "sourceId": 11755367,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7379931,
     "sourceId": 11755527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7404191,
     "sourceId": 11791810,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
